---
title: "Coursera Practical Machine Learning"
output: html_document
---
*Ousmane Souleymane*  
*June 07, 2016*
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
The objective of this analysis is to attempt to identify the type of exercise when using device such as Jawbone Up, Nike FuelBand and FitBit.  
The required data for the processing are collected from motion sensors placed on 6 participants. The accelerometers were placed on the arm, forearm, belt and dumbell. 

## Data processing
For the analysis we need some R packages as indicated in the code.  
The training dataset contain 19622 rows of data with 160 variables and the testing dataset contain 20 observations.
```{r}
# Required R packages
library(caret) 
library(rattle) 
library(rpart) 
library(rpart.plot)
library(randomForest)
library(repmis)
```

```{r}
# Load the dataset
training <- read.csv("pml-training.csv", na.strings = c("NA", ""))
testing <- read.csv("pml-testing.csv", na.strings = c("NA", ""))
```


## Data cleaning
We have note that many variables contained a lot of missing value so they were removed from the dataset.
```{r}
# Delete colums with missing values
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
```

```{r}
# Remove some predictors 
trainData <- training[, -c(1:7)]
testData <- testing[, -c(1:7)]
```

## Data splitting
The dataset was partitioned into training and testing with 70% going for training and 30% for the testing.
```{r}
# Data splitting
set.seed(7826) 
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)
train <- trainData[inTrain, ]
valid <- trainData[-inTrain, ]
```


## Model
Many test were conducted with some classification method (multinomial logistic regression, niave Bayes, decision trees, etc). The decision trees and random forest method produced the best result to predict the model.
```{r}
# Classification trees
control <- trainControl(method = "cv", number = 5)
fit_rpart <- train(classe ~ ., data = train, method = "rpart", 
                   trControl = control)
print(fit_rpart, digits = 4)
```

```{r}
fancyRpartPlot(fit_rpart$finalModel)
```

## Decision trees 
To fit the decision trees, we used the caret package to conduct 10-fold cross validation.
Despite the 10-fold cross validation process, the confusion matrix indicates an accuracy rate of 0.5 and the out-of-sample error rate is 0.5. It's likely that we can find a model that can give good prediction.

```{r}
# predict outcomes using validation set
predict_rpart <- predict(fit_rpart, valid)
# Show prediction result
(conf_rpart <- confusionMatrix(valid$classe, predict_rpart))
```

```{r}
(accuracy_rpart <- conf_rpart$overall[1])
```

## Random forest
The accuracy rate generate by the random forest model is 0.991, so no additional cross validation is necessary. 
```{r}
fit_rf <- train(classe ~ ., data = train, method = "rf", 
                   trControl = control)
print(fit_rf, digits = 4)
```

```{r}
# predict outcomes using validation set
predict_rf <- predict(fit_rf, valid)
# Show prediction result
(conf_rf <- confusionMatrix(valid$classe, predict_rf))
```

```{r}
(accuracy_rf <- conf_rf$overall[1])
```

## Conclusion
The random forest is well suited for the generating accurate prediction for this dataset. Complete the course, we use this method to predict the outcome variable classe for the 20 tests cases.

```{r}
(predict(fit_rf, testData))
```

